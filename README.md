# ğŸ‘¨â€ğŸ’»âš–ï¸tcesp-crawler

## Autor
**Jorge Luiz Figueira**  
[LinkedIn](https://www.linkedin.com/in/jorgeluizfigueira/)

## ğŸ“Œ Contexto
Este projeto foi desenvolvido como parte do desafio tÃ©cnico para a vaga de **Desenvolvedor Python Jr. (Crawler/RPA)** na [Turivius](https://turivius.com/). O objetivo principal Ã© criar um **robÃ´ de automaÃ§Ã£o de processos (RPA)** capaz de extrair dados do site do **TCE-SP**. Embora o site ofereÃ§a diversos parÃ¢metros de busca para os processos, este desafio focou especificamente no parÃ¢metro *"Todas essas palavras" (txtTdPalvs)*, utilizando como exemplo o termo: *"fraude em escolas"*. 

## ğŸ“œ Desafio e CritÃ©rios de AvaliaÃ§Ã£o
O robÃ´ deve:
- Extrair corretamente os dados do site do **TCE-SP**.
- Utilizar **requests_html, Selenium ou Scrapy**.
- Salvar os dados em um banco de dados **(MongoDB, PostgreSQL, MySQL, etc.)**.
- Garantir boa performance na raspagem e recuperaÃ§Ã£o de informaÃ§Ãµes.
- Ser estruturado para ambiente de produÃ§Ã£o, utilizando **Docker e Docker Compose**.
- Seguir boas prÃ¡ticas de cÃ³digo (**PEP8**), documentaÃ§Ã£o clara e tratamento de exceÃ§Ãµes.
- Diferenciais: uso de **requests_html ou bs4**, entrega containerizada e armazenamento em **MongoDB**.

## ğŸ“‚ Estrutura do RepositÃ³rio
```
/
â”œâ”€â”€ crawler/                # MÃ³dulo principal do Crawler
â”‚   â”œâ”€â”€ main.py             # CÃ³digo de extraÃ§Ã£o de dados
â”‚   â”œâ”€â”€ __init__.py         # Arquivo de inicializaÃ§Ã£o do mÃ³dulo
â”‚
â”œâ”€â”€ db/                     # ConfiguraÃ§Ã£o do banco de dados
â”‚   â”œâ”€â”€ db_config.py        # ConfiguraÃ§Ã£o do MongoDB
â”‚
â”œâ”€â”€ .env                    # VariÃ¡veis de ambiente
â”œâ”€â”€ Dockerfile              # DefiniÃ§Ã£o do container
â”œâ”€â”€ docker-compose.yml      # OrquestraÃ§Ã£o de serviÃ§os
â”œâ”€â”€ requirements.txt        # DependÃªncias do projeto
â”œâ”€â”€ README.md               # DocumentaÃ§Ã£o do projeto
```

## ğŸ” SoluÃ§Ã£o Implementada
- O projeto foi **containerizado** com **Docker e Docker Compose**.
- Utiliza **MongoDB** para armazenamento dos dados extraÃ­dos.
- O web crawler estÃ¡ configurado para buscar automaticamente processos com o termo **"fraude em escolas"**.

## ğŸš€ Como Executar o Projeto

### 1ï¸âƒ£ Configurar VariÃ¡veis de Ambiente
Crie um arquivo **.env** na raiz do projeto com as seguintes configuraÃ§Ãµes:

```ini
MONGO_USER=YOUR-USER-NAME
MONGO_PASSWORD=YOUR-DB-PASSWORD
MONGO_HOST=YOUR-HOST
MONGO_PORT=27017
MONGO_DB=YOUR-DB-NAME
MONGO_COLLECTION=YOUR-COLLECTION
```

### 2ï¸âƒ£ Construir e Rodar os Containers
Execute os comandos abaixo no terminal:

```sh
docker-compose up --build
```
Isso iniciarÃ¡ tanto o banco de dados **MongoDB** quanto o **crawler**.

### 3ï¸âƒ£ Iniciar a Raspagem Manualmente (Opcional)
Caso queira rodar manualmente o crawler dentro do container:

```sh
docker exec -it tcesp-crawler python crawler/main.py
```

O script buscarÃ¡ processos com o termo **"fraude em escolas"** e armazenarÃ¡ os dados no MongoDB.

---
